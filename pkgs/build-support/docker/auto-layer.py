#!/usr/bin/env python3

# usage: auto-layer.py graph_file [ignore_file] [layer_limit]

# graph_file: Path to a json file as generated by writeReferencesGraph
# ignore_file: Path to a file with a list of store paths that should not appear in the output
# layer_limit: Maximum number of layers to generate, default 100

# This module tries to split a dependency graph of nix store paths into a
# limited set of layers that together cover all mentioned paths. It tries to
# choose the layers such that different inputs often have the largest layers in
# common so most layers can be shared, while the differences in the results end
# up in smaller layers.

# It does this by splitting off the N largest store paths (by nar size) into
# their own layers, including some of their dependencies.
# Specifically, for a large store path L, it creates a layer with L and any
# store path D that L depends on and for which there is no store path in the
# input that depends on D but not on L.
# Then, if there are any store paths that are depended on by multiple of the
# chosen large store paths, those common dependencies will get their own layer,
# one per set of large store paths that depends on them.
# N is iteratively increased until the layer limit is reached.

# The reasoning for this algorithm is as follows:

# Most closures contain a few large store paths and many small store paths. If
# we want to share as many bytes as possible with other layered images, we
# should focus on putting the largest paths in their own layer.

# If we had data on how much each store path is used and how likely each
# combination of store paths is, we might be able to infer which large store
# paths are better off being combined into a single layer. However, getting that
# information, let alone keeping it up-to-date is very difficult. If we can't
# tell that two large store paths are often going to appear together, then we're
# better off giving each of them their own layer.

# This leaves a lot of smaller store paths to be assigned to layers. Anything
# that will depend on a large store path L will also depend on all the store
# paths that L depends on, so it makes sense to move the dependencies of L into
# the same layer as L.

# Possible improvements:
# - Specifying a size limit below which the algorithm stops using large store
#   paths as new layer roots might further improve sharing as the layer
#   boundaries will depend less on the number of larger store paths in the
#   input.

import os
import sys
import json

def layer_count(layer_split):
    return len(set(layer_split.values()))

def path_key(path):
    hash, name = path.split('-', 1)
    return name, hash

def closure(*todo, key):
    """
    Find all dependencies of the arguments including the arguments themselves.
    """
    todo = set(todo)
    done = set()
    while todo:
        x = todo.pop()
        if x not in done:
            done.add(x)
            todo.update(key(x))
    return done

def dependencies(*todo, key):
    """
    Find all dependencies of the arguments excluding the arguments themselves.
    """
    return closure(*todo, key=key) - set(todo)

def minimal_cover(paths, key):
    """
    The minimal set of paths that together cover all input paths with their
    closure. None of the result paths depend on each other.
    """
    paths = set(paths)
    paths_deps = set.union(*(dependencies(d, key=key) for d in paths))
    return paths - paths_deps

graph_file = sys.argv[1]
ignore_file = sys.argv[2] if len(sys.argv) > 2 else "/dev/null"
layer_limit = int(sys.argv[3]) if len(sys.argv) > 3 else 100
with open(graph_file) as f:
    graph = json.load(f)
with open(ignore_file) as f:
    ignore_paths = {line.strip() for line in f}

# Compute all direct users of each path
nodes = {x["path"]: x | {"users": set()} for x in graph}
for user in nodes:
    for ref in nodes[user]["references"]:
        nodes[ref]["users"] |= {user}

def node_deps(path):
    return nodes[path]["references"]

def node_users(path):
    return nodes[path]["users"]

nodes_by_size = sorted(graph, key=lambda node: node["narSize"])

# GOAL: Every path in a layer has the same set of other layers that depend on it.

# Map from a path to the set of primary paths that depend on it and do not depend
# on any other primary path that depends on it.
layer_split = {path: frozenset() for path in nodes}
primary_paths = set()
while nodes_by_size:
    # STEP: The next biggest path will be the root of a new layer.
    new_primary_path = nodes_by_size.pop()["path"]
    primary_paths.add(new_primary_path)
    new_layer_split = {k: set(v) for k, v in layer_split.items()}
    new_layer_split[new_primary_path] = {new_primary_path}
    new_primary_path_deps = dependencies(new_primary_path, key=node_deps)
    new_primary_path_users = dependencies(new_primary_path, key=node_users)
    for dep in new_primary_path_deps:
        new_layer_split[dep] -= new_primary_path_users
        if not new_layer_split[dep] & new_primary_path_deps:
            new_layer_split[dep] |= {new_primary_path}
    new_layer_split = {k: frozenset(v) for k, v in new_layer_split.items()}
    if layer_count(new_layer_split) > layer_limit:
        break
    layer_split = new_layer_split

def layer_info(layer_id):
    paths = {path
             for path, layer_id_2 in layer_split.items()
             if layer_id == layer_id_2}
    layerSize = sum(nodes[path]["narSize"] for path in paths)
    deps = dependencies(*paths, key=node_deps)
    return {
        "usedBy": sorted(layer_id, key=path_key),
        "paths": sorted(paths, key=path_key),
        "layerSize": layerSize,
        "closureSize": sum(nodes[path]["narSize"] for path in closure(*paths, key=node_deps)),
    }

layers = {layer_id: layer_info(layer_id)
          for layer_id in set(layer_split.values())}

layer_order = sorted(layers.values(), key=lambda info: info["closureSize"])

if os.environ.get("DEBUG"):
    print(json.dumps(layer_order, indent=2), file=sys.stderr)

total_layer_size = sum(node["layerSize"] for node in layer_order)
total_nar_size = sum(node["narSize"] for node in graph)
assert total_layer_size == total_nar_size, (total_layer_size, total_nar_size)

result = [[path
           for path in layer["paths"]
           if path not in ignore_paths]
          for layer in layer_order
          if set(layer["paths"]) - ignore_paths]

print(json.dumps(result))
